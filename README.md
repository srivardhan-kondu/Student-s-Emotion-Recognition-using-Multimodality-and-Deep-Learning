# ğŸ“ Student's Emotion Recognition using Multimodality and Deep Learning

> An AI-powered system that detects student emotions by analyzing **facial expressions**, **speech signals**, and **text input** simultaneously â€” then **fuses** all three predictions into a single, reliable emotion output using advanced decision strategies.

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [How It Works (Simple Explanation)](#-how-it-works-simple-explanation)
- [System Architecture](#-system-architecture)
- [Model Details](#-model-details)
  - [Facial Emotion Recognition (CNN)](#1-facial-emotion-recognition-cnn)
  - [Speech Emotion Recognition (Attention-BiLSTM)](#2-speech-emotion-recognition-attention-bilstm)
  - [Text Emotion Recognition (BERT)](#3-text-emotion-recognition-bert)
- [Multimodal Fusion Engine](#-multimodal-fusion-engine)
  - [Calibrated Fusion (Default)](#1-calibrated-fusion-default--recommended)
  - [Weighted Fusion](#2-weighted-fusion)
  - [Adaptive Fusion](#3-adaptive-fusion)
  - [Voting Fusion](#4-voting-fusion)
- [Training Parameters](#-training-parameters)
- [Model Performance](#-model-performance)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)
- [Dashboard Guide](#-dashboard-guide)
- [Python API](#-python-api)
- [Datasets](#-datasets)
- [Technology Stack](#-technology-stack)
- [Functional Requirements](#-functional-requirements)
- [Documentation](#-documentation)

---

## ğŸ¯ Overview

This system recognizes **6 emotions** from students:

| Emotion | Example |
|---------|---------|
| ğŸ˜Š **Happy** | Smiling face, cheerful voice, positive text |
| ğŸ˜¢ **Sad** | Downturned lips, low monotone voice, negative text |
| ğŸ˜  **Angry** | Furrowed brows, loud/harsh voice, aggressive text |
| ğŸ˜ **Neutral** | Relaxed face, calm voice, factual text |
| ğŸ˜¨ **Fear** | Wide eyes, trembling voice, anxious text |
| ğŸ˜² **Surprise** | Raised eyebrows, sudden pitch change, unexpected text |

### Why Multimodal?

A single modality can be misleading. Someone might **smile** while saying something **sarcastic** (text says negative). By combining all three modalities, the system is much more accurate and robust than any single model alone.

---

## ğŸ§  How It Works (Simple Explanation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“· Image   â”‚â”€â”€â”€â”€â–¶â”‚  Facial CNN Model    â”‚â”€â”€â”€â”€â–¶â”‚ "happy 85%" â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤ Audio   â”‚â”€â”€â”€â”€â–¶â”‚  Speech BiLSTM Model â”‚â”€â”€â”€â”€â–¶â”‚ "happy 92%" â”‚â”€â”€â”¼â”€â”€â–¶â”‚  ğŸ”€ FUSION   â”‚â”€â”€â–¶ Final: HAPPY (89%)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚    ENGINE    â”‚
                                                                   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  ğŸ“ Text    â”‚â”€â”€â”€â”€â–¶â”‚  BERT Text Model     â”‚â”€â”€â”€â”€â–¶â”‚ "happy 98%" â”‚â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step-by-step:**

1. **Input** â€” User uploads an image, audio file, and/or enters text
2. **Individual Models** â€” Each modality's deep learning model independently predicts an emotion
3. **Calibration** â€” Raw model probabilities are *temperature-scaled* so overconfident models don't dominate
4. **Fusion** â€” The fusion engine combines all predictions using configurable strategies
5. **Output** â€” A single final emotion with confidence score and breakdown per modality

---

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ Input Layer"]
        IMG["ğŸ“· Image\n(JPG/PNG)"]
        AUD["ğŸ¤ Audio\n(WAV/MP3)"]
        TXT["ğŸ“ Text\n(String)"]
    end

    subgraph PreProcess["ğŸ”§ Preprocessing"]
        FD["Face Detector\n(Haar Cascade)"]
        AF["Audio Feature\nExtractor (MFCC)"]
        TP["Text Preprocessor\n(Tokenization + Cleaning)"]
    end

    subgraph Models["ğŸ§  Deep Learning Models"]
        CNN["Facial CNN\n(MiniXception)\n48Ã—48 grayscale"]
        LSTM["Speech BiLSTM\n(Attention-based)\nMFCC features"]
        BERT["Text BERT\n(Fine-tuned)\n128 token max"]
    end

    subgraph Fusion["ğŸ”€ Fusion Engine"]
        CAL["Temperature\nCalibration"]
        GATE["Confidence\nGating"]
        FUSE["Weighted\nCombination"]
    end

    OUT["ğŸ¯ Final Emotion\n+ Confidence Score"]

    IMG --> FD --> CNN
    AUD --> AF --> LSTM
    TXT --> TP --> BERT

    CNN --> CAL
    LSTM --> CAL
    BERT --> CAL

    CAL --> GATE --> FUSE --> OUT
```

---

## ğŸ”¬ Model Details

### 1. Facial Emotion Recognition (CNN)

The facial model uses a **MiniXception** architecture â€” a lightweight CNN designed specifically for real-time emotion detection on small grayscale images.

#### Architecture

```
Input Image (48 Ã— 48 Ã— 1 grayscale)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv2D(8, 3Ã—3) + BN + ReLU    â”‚  â† Entry flow: basic feature extraction
â”‚ Conv2D(8, 3Ã—3) + BN + ReLU    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RESIDUAL BLOCK Ã—4 â”‚  â† 4 blocks with increasing filters
    â”‚                    â”‚
    â”‚  Filters: 16 â†’ 32 â†’ 64 â†’ 128
    â”‚                    â”‚
    â”‚  Each block:       â”‚
    â”‚  â”œâ”€ SeparableConv2D(f, 3Ã—3) + BN + ReLU
    â”‚  â”œâ”€ SeparableConv2D(f, 3Ã—3) + BN + ReLU
    â”‚  â”œâ”€ MaxPool(3Ã—3, stride=2)
    â”‚  â””â”€ + Residual Connection (1Ã—1 conv shortcut)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    Conv2D(6, 3Ã—3)           â† 6 filters = 6 emotion classes
    GlobalAveragePooling2D
    Softmax
              â”‚
              â–¼
    Output: [happy, sad, angry, neutral, fear, surprise]
```

#### Key Design Decisions

| Decision | Choice | Why |
|----------|--------|-----|
| **Input size** | 48Ã—48 grayscale | FER2013 standard; keeps model small and fast |
| **Separable convolutions** | Depthwise-separable | 8-9Ã— fewer parameters than regular Conv2D |
| **Residual connections** | Skip connections | Prevents vanishing gradients in deeper blocks |
| **No fully connected layers** | GAP instead | Reduces overfitting, forces spatial learning |
| **Face detection** | Haar Cascade (OpenCV) | Fast, works well for frontal faces |

#### Training Configuration

| Parameter | Value |
|-----------|-------|
| Optimizer | Adam (lr=0.001) |
| Loss | Categorical Crossentropy |
| Batch size | 32 |
| Epochs | 50 |
| Input shape | 48 Ã— 48 Ã— 1 |
| Data split | 15% test, 15% val |

---

### 2. Speech Emotion Recognition (Attention-BiLSTM)

The speech model uses an **Attention-based Bidirectional LSTM** â€” designed to capture both short-range audio patterns (via CNNs) and long-range temporal dependencies (via BiLSTMs), with an attention mechanism that focuses on the most emotionally relevant parts of the audio.

#### Audio Feature Extraction (Preprocessing)

Before the audio reaches the model, it goes through feature extraction:

```
Raw Audio (.wav) â”€â”€â–¶ librosa â”€â”€â–¶ MFCC Features
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚             â”‚             â”‚
                  MFCC(40)    Delta(40)    DeltaÂ²(40)
                    â”‚             â”‚             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                          Feature Vector
                        (time_steps Ã— 120)
```

| Feature | Count | What It Captures |
|---------|-------|-----------------|
| **MFCC** | 40 coefficients | Spectral envelope (pitch, timbre) |
| **Delta MFCC** | 40 coefficients | Rate of change (intonation shifts) |
| **Delta-Delta MFCC** | 40 coefficients | Acceleration (sudden emotion changes) |
| **Total** | **120 features** per time frame | |

| Audio Parameter | Value |
|----------------|-------|
| Sample rate | 22,050 Hz |
| FFT window | 2,048 samples |
| Hop length | 512 samples |

#### Model Architecture

```
MFCC Input (T Ã— 120)     â† T = time steps, 120 = MFCC + deltas
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv1D(64, k=3) + BN + ReLU     â”‚  â† Short-range feature extraction
â”‚  Conv1D(64, k=3) + BN + ReLU     â”‚
â”‚  MaxPool1D(2) + Dropout(0.2)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bidirectional LSTM(128)          â”‚  â† Forward + backward temporal patterns
â”‚  Dropout(0.3)                     â”‚
â”‚  Bidirectional LSTM(64)           â”‚  â† Refined temporal encoding
â”‚  Dropout(0.3)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SELF-ATTENTION LAYER             â”‚  â† Learns which time steps matter most
â”‚  (attention weights over T axis)  â”‚     for emotion classification
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dense(256) + BN + Dropout(0.4)   â”‚
â”‚  Dense(128) + Dropout(0.3)        â”‚
â”‚  Dense(6, softmax)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    Output: [happy, sad, angry, neutral, fear, surprise]
```

#### How Self-Attention Works (Simple)

The attention mechanism assigns an **importance score** to each time frame of the audio:

```
Time:     [0.1s]  [0.2s]  [0.3s]  [0.4s]  [0.5s]  [0.6s]
Audio:    silence  "I'm"   "so"   "ANGRY"  "at"    "you"
Attention: 0.05    0.10    0.15    0.40     0.15    0.15
                                    â†‘
                          Model focuses HERE (loud, emphatic)
```

Instead of treating all time steps equally, the model **focuses** on the most emotionally expressive parts of the speech.

#### Training Configuration

| Parameter | Value |
|-----------|-------|
| Optimizer | Adam (lr=0.001) |
| Loss | Categorical Crossentropy (label smoothing=0.1) |
| Batch size | 32 |
| Epochs | 80 (with EarlyStopping) |
| Label smoothing | 0.1 (prevents overconfidence) |

---

### 3. Text Emotion Recognition (BERT)

The text model uses **BERT** (Bidirectional Encoder Representations from Transformers) â€” a pre-trained transformer model fine-tuned on emotion classification.

#### How BERT Works (Simple)

Unlike traditional models that read text left-to-right, BERT reads **both directions simultaneously**:

```
Traditional:  "I" â†’ "love" â†’ "this" â†’ "class"  (only sees past words)

BERT:         "I" â† "love" â†’ "this" â† "class"  (sees ALL words at once)
                       â†•              â†•
              Full bidirectional context
```

This means when BERT sees the word "love", it already knows it's followed by "this class" â€” giving much richer understanding.

#### Architecture

```
Input Text: "I really enjoyed today's lecture!"
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BERT Tokenizer                   â”‚  â† Converts text to token IDs
â”‚  [CLS] I really enjoyed ...       â”‚     Adds special tokens
â”‚  Max length: 128 tokens           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BERT-base-uncased                â”‚  â† 12 transformer layers
â”‚  (110M parameters)                â”‚     768 hidden dimensions
â”‚  Pre-trained on English Wikipedia â”‚     12 attention heads
â”‚  + BookCorpus                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [CLS] Token Embedding            â”‚  â† Represents whole sentence meaning
â”‚                                    â”‚
â”‚  Classification Head:              â”‚
â”‚  Linear(768 â†’ 6) + Softmax        â”‚  â† Maps to 6 emotions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    Output: [happy, sad, angry, neutral, fear, surprise]
```

#### BERT Parameters

| Parameter | Value |
|-----------|-------|
| Model | `bert-base-uncased` |
| Hidden size | 768 |
| Attention heads | 12 |
| Transformer layers | 12 |
| Total parameters | ~110M |
| Max token length | 128 |
| Optimizer | AdamW (lr=2Ã—10â»âµ) |
| Batch size | 16 |
| Epochs | 5 |
| Fine-tuned layers | Classifier head + top BERT layers |

#### Temperature Scaling

BERT models tend to be **overconfident** (predicting 99% for one class). Temperature scaling softens the output:

```
Before calibration (T=1.0):  [0.95, 0.01, 0.01, 0.01, 0.01, 0.01]  â† Overconfident!
After calibration  (T=1.2):  [0.78, 0.05, 0.05, 0.04, 0.04, 0.04]  â† More realistic
```

| Temperature | Effect |
|-------------|--------|
| T = 1.0 | No change (raw softmax) |
| T > 1.0 | Softens predictions (reduces overconfidence) |
| T < 1.0 | Sharpens predictions (increases confidence) |

**Our calibration temperatures:**
- Facial: T = 1.5 (most overconfident â†’ soften the most)
- Speech: T = 1.3
- Text: T = 1.2

---

## ğŸ”€ Multimodal Fusion Engine

The fusion engine is the **brain** of the system. It takes independent predictions from each modality and combines them into a single, reliable prediction. The system supports **4 fusion strategies**:

### 1. Calibrated Fusion (Default â€” Recommended)

This is the most sophisticated strategy. It performs three steps:

```
Step 1: CALIBRATE each modality's probabilities
        (apply temperature scaling to prevent overconfident models from dominating)

Step 2: GATE low-confidence modalities
        (if a modality's confidence < 30%, skip it entirely)

Step 3: WEIGHT & COMBINE
        (multiply calibrated probabilities by confidence-scaled weights, then normalize)
```

**Formula:**

```
effective_weight[m] = base_weight[m] Ã— confidence[m]

fused_probs = Î£ (effective_weight[m] Ã— calibrated_probs[m]) / Î£ effective_weight[m]
```

**Example:**
```
Facial:  happy=80%, confidence=0.80 â†’ effective weight = 0.40 Ã— 0.80 = 0.32
Speech:  happy=90%, confidence=0.95 â†’ effective weight = 0.30 Ã— 0.95 = 0.285
Text:    happy=20%, confidence=0.25 â†’ SKIPPED (below 30% threshold)

Final = (0.32 Ã— facial_probs + 0.285 Ã— speech_probs) / (0.32 + 0.285)
```

### 2. Weighted Fusion

Simple linear combination with fixed weights:

```
fused = 0.40 Ã— facial + 0.30 Ã— speech + 0.30 Ã— text
```

Pros: Simple, predictable. Cons: Ignores confidence levels.

### 3. Adaptive Fusion

Like weighted fusion, but weights are dynamically adjusted by each modality's confidence:

```
adaptive_weight[m] = base_weight[m] Ã— confidence[m]
fused = Î£ (adaptive_weight[m] Ã— probs[m]) / Î£ adaptive_weight[m]
```

A modality that is very confident gets amplified; a low-confidence modality gets diminished.

### 4. Voting Fusion

Each modality casts a "vote" for its top predicted emotion. The emotion with the most votes wins:

```
Facial predicts: HAPPY    â†’ +1 vote for HAPPY
Speech predicts: HAPPY    â†’ +1 vote for HAPPY
Text predicts:   NEUTRAL  â†’ +1 vote for NEUTRAL

Result: HAPPY wins (2 vs 1)
```

Pros: Simple, robust to outliers. Cons: Ignores probability magnitudes.

### Fusion Comparison

| Strategy | Uses Calibration | Uses Confidence | Handles Missing Modalities | Best For |
|----------|:---:|:---:|:---:|---------|
| **Calibrated** | âœ… | âœ… | âœ… | Production use (most accurate) |
| **Weighted** | âŒ | âŒ | âœ… | Simple, fast predictions |
| **Adaptive** | âŒ | âœ… | âœ… | Variable-quality inputs |
| **Voting** | âŒ | âŒ | âœ… | Quick consensus |

### Default Modality Weights

| Modality | Weight | Rationale |
|----------|--------|-----------|
| Facial | 0.40 | Strongest visual signal for basic emotions |
| Speech | 0.30 | Captures tone, pitch, and energy |
| Text | 0.30 | Captures semantic meaning and context |

These weights can be adjusted in real-time via the dashboard sidebar sliders.

---

## âš™ï¸ Training Parameters

### Complete Parameter Reference

| Parameter | Facial | Speech | Text |
|-----------|--------|--------|------|
| **Framework** | TensorFlow/Keras | TensorFlow/Keras | PyTorch (HuggingFace) |
| **Architecture** | MiniXception CNN | Attention-BiLSTM | BERT-base-uncased |
| **Input format** | 48Ã—48Ã—1 grayscale | MFCC time-series | Tokenized text (128 max) |
| **Optimizer** | Adam | Adam | AdamW |
| **Learning rate** | 0.001 | 0.001 | 2Ã—10â»âµ |
| **Batch size** | 32 | 32 | 16 |
| **Max epochs** | 50 | 80 | 5 |
| **Early stopping** | âœ… (patience=10) | âœ… (patience=15) | âœ… (best val accuracy) |
| **Loss function** | Categorical CE | Cat. CE + Label Smoothing(0.1) | Cross Entropy |
| **Data split** | 70/15/15 | 70/15/15 | 70/15/15 |
| **Random seed** | 42 | 42 | 42 |

---

## ğŸ“Š Model Performance

### Accuracy Results

| Model | Training Accuracy | Validation Accuracy | Dataset |
|-------|:-:|:-:|---------|
| **Facial (CNN)** | ~60% | **57.7%** | FER2013 (35,887 images) |
| **Speech (BiLSTM)** | ~98% | **97.0%** | RAVDESS (1,440 audio files) |
| **Text (BERT)** | ~70% | **65.9%** | GoEmotions (58,000 texts) |

> **Note:** FER2013 is a notoriously difficult dataset â€” 57.7% is typical for lightweight models. State-of-the-art reaches ~73% with much larger architectures. The speech model achieves 97% because RAVDESS is a controlled, acted dataset with clear emotional expressions.

### Training History Visualizations

Training history plots and confusion matrices are available in the [`docs/`](docs/) directory:
- Facial model: training/validation accuracy curves + confusion matrix
- Speech model: training/validation accuracy curves + confusion matrix

---

## ğŸ“ Project Structure

```
Student's Emotion Recognition using Multimodality and Deep Learning/
â”‚
â”œâ”€â”€ src/                                # ğŸ”§ Source code
â”‚   â”œâ”€â”€ config.py                       #    Global configuration & hyperparameters
â”‚   â”œâ”€â”€ facial_recognition/             #    ğŸ‘ï¸ Facial emotion module
â”‚   â”‚   â”œâ”€â”€ model_architecture.py       #       CNN architectures (MiniXception, EfficientNet)
â”‚   â”‚   â”œâ”€â”€ emotion_model.py            #       EmotionCNN wrapper class
â”‚   â”‚   â”œâ”€â”€ face_detector.py            #       Face detection (Haar Cascade)
â”‚   â”‚   â”œâ”€â”€ data_preprocessing.py       #       Image augmentation & loading
â”‚   â”‚   â””â”€â”€ train.py                    #       Training script
â”‚   â”œâ”€â”€ speech_analysis/                #    ğŸ¤ Speech emotion module
â”‚   â”‚   â”œâ”€â”€ emotion_model.py            #       Attention-BiLSTM model
â”‚   â”‚   â”œâ”€â”€ audio_features.py           #       MFCC feature extraction
â”‚   â”‚   â”œâ”€â”€ speech_recognition.py       #       Audio loading utilities
â”‚   â”‚   â””â”€â”€ train.py                    #       Training script
â”‚   â”œâ”€â”€ text_analysis/                  #    ğŸ“ Text emotion module
â”‚   â”‚   â”œâ”€â”€ emotion_model.py            #       BERT/RoBERTa classifiers + FocalLoss
â”‚   â”‚   â”œâ”€â”€ text_preprocessing.py       #       Text cleaning & tokenization
â”‚   â”‚   â””â”€â”€ train.py                    #       Training script
â”‚   â”œâ”€â”€ fusion/                         #    ğŸ”€ Multimodal fusion module
â”‚   â”‚   â”œâ”€â”€ multimodal_fusion.py        #       4 fusion strategies + temperature calibration
â”‚   â”‚   â””â”€â”€ multimodal_predictor.py     #       High-level prediction API
â”‚   â”œâ”€â”€ dashboard/                      #    ğŸ–¥ï¸ Web interface
â”‚   â”‚   â”œâ”€â”€ app.py                      #       Streamlit dashboard (main UI)
â”‚   â”‚   â””â”€â”€ run.py                      #       Dashboard launcher
â”‚   â””â”€â”€ utils/                          #    ğŸ› ï¸ Utility functions
â”‚       â”œâ”€â”€ helpers.py                  #       Common helpers
â”‚       â””â”€â”€ voice_recorder.py           #       Audio recording utility
â”‚
â”œâ”€â”€ saved_models/                       # ğŸ’¾ Trained model weights (gitignored)
â”‚   â”œâ”€â”€ facial_emotion_model.h5         #    ~43 MB
â”‚   â”œâ”€â”€ speech_emotion_model.h5         #    ~6 MB
â”‚   â””â”€â”€ text_bert_model/                #    ~438 MB (HuggingFace format)
â”‚
â”œâ”€â”€ data/                               # ğŸ“¦ Datasets (gitignored â€” download separately)
â”œâ”€â”€ docs/                               # ğŸ“š Documentation + training plots
â”œâ”€â”€ tests/                              # ğŸ§ª Unit tests
â”œâ”€â”€ requirements.txt                    # ğŸ“‹ Python dependencies
â”œâ”€â”€ run_dashboard.py                    # â–¶ï¸ Dashboard launcher script
â”œâ”€â”€ QUICKSTART.md                       # ğŸš€ Quick start guide
â”œâ”€â”€ DATASET_INSTRUCTIONS.md             # ğŸ“¥ Dataset download instructions
â””â”€â”€ README.md                           # ğŸ“– This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python** 3.8 or higher
- **pip** package manager
- **~2 GB** disk space (for models + dependencies)
- *(Optional)* CUDA-capable GPU for faster training

### Step 1: Clone the Repository

```bash
git clone https://github.com/srivardhan-kondu/Student-s-Emotion-Recognition-using-Multimodality-and-Deep-Learning.git
cd "Student's Emotion Recognition using Multimodality and Deep Learning"
```

### Step 2: Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate        # macOS/Linux
# venv\Scripts\activate         # Windows
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
```

### Step 4: Download Datasets

| Dataset | Modality | Size | Download Link |
|---------|----------|------|---------------|
| **FER2013** | Facial | ~300 MB | [Kaggle](https://www.kaggle.com/datasets/msambare/fer2013) |
| **RAVDESS** | Speech | ~1.1 GB | [Kaggle](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio) |
| **GoEmotions** | Text | ~50 MB | [Kaggle](https://www.kaggle.com/datasets/debarshichanda/goemotions) |

Place them in the `data/` directory:

```
data/
â”œâ”€â”€ facial/fer2013/          # FER2013 images (train/ and test/ subfolders)
â”œâ”€â”€ speech/ravdess/          # RAVDESS audio files
â””â”€â”€ text/goemotions/         # GoEmotions CSV files
```

See [DATASET_INSTRUCTIONS.md](DATASET_INSTRUCTIONS.md) for detailed steps.

### Step 5: Train Models

```bash
# Train facial model (~30 min on CPU, ~5 min on GPU)
python src/facial_recognition/train.py

# Train speech model (~20 min on CPU)
python src/speech_analysis/train.py

# Train text model (~40 min on CPU, ~10 min on GPU)
python src/text_analysis/train.py
```

### Step 6: Launch Dashboard

```bash
python run_dashboard.py
```

Open your browser at **http://localhost:8501** ğŸ‰

---

## ğŸ–¥ï¸ Dashboard Guide

The Streamlit dashboard provides 4 tabs:

| Tab | What It Does |
|-----|-------------|
| **ğŸ¯ Multimodal** | Upload image + audio + text â†’ fused prediction |
| **ğŸ‘ï¸ Image** | Upload/capture image â†’ facial emotion only |
| **ğŸ¤ Audio** | Upload audio â†’ speech emotion only |
| **ğŸ“ Text** | Enter text â†’ text emotion only |

### Sidebar Controls

- **Fusion Strategy** â€” Switch between Calibrated, Weighted, Adaptive, or Voting
- **Modality Weights** â€” Adjust facial/speech/text weights with sliders (for Weighted and Adaptive modes)
- **Prediction History** â€” Shows last 5 predictions

---

## ğŸ Python API

You can use the system programmatically without the dashboard:

```python
from src.fusion.multimodal_predictor import MultimodalEmotionPredictor

# Initialize and load models
predictor = MultimodalEmotionPredictor()
predictor.load_models()

# Single modality predictions
facial_result = predictor.predict_from_image("photo.jpg")
speech_result = predictor.predict_from_audio("speech.wav")
text_result   = predictor.predict_from_text("I'm so happy today!")

# Multimodal prediction (any combination works)
result = predictor.predict_multimodal(
    image_path="photo.jpg",
    audio_path="speech.wav",
    text="I'm so happy today!"
)

print(f"Emotion: {result['emotion']}")
print(f"Confidence: {result['confidence']:.1%}")
print(f"Modalities Used: {result['modalities_used']}/3")

# Access individual modality results
for modality, res in result['individual_results'].items():
    print(f"  {modality}: {res['emotion']} ({res['confidence']:.1%})")
```

---

## ğŸ“¦ Datasets

### FER2013 (Facial)

| Property | Value |
|----------|-------|
| Total images | 35,887 |
| Image size | 48 Ã— 48 pixels, grayscale |
| Classes | 7 (mapped to our 6 emotions) |
| Source | Facial Expression Recognition challenge |

### RAVDESS (Speech)

| Property | Value |
|----------|-------|
| Total files | 1,440 audio clips |
| Actors | 24 (12 male, 12 female) |
| Format | WAV, 48 kHz |
| Emotions | 8 (mapped to our 6 emotions) |
| Source | Ryerson Audio-Visual Database |

### GoEmotions (Text)

| Property | Value |
|----------|-------|
| Total texts | ~58,000 Reddit comments |
| Classes | 27 fine-grained (mapped to our 6 emotions) |
| Language | English |
| Source | Google Research |

---

## ğŸ› ï¸ Technology Stack

| Category | Technologies | Purpose |
|----------|-------------|---------|
| **Deep Learning** | TensorFlow 2.x, PyTorch, HuggingFace Transformers | Model training & inference |
| **Computer Vision** | OpenCV, Haar Cascades | Face detection & image processing |
| **Audio Processing** | librosa | MFCC feature extraction |
| **NLP** | NLTK, HuggingFace Tokenizers | Text preprocessing & tokenization |
| **Web Framework** | Streamlit | Interactive dashboard |
| **Visualization** | Matplotlib, Plotly, Seaborn | Training plots & result charts |
| **Data Science** | NumPy, Pandas, scikit-learn | Data manipulation & evaluation |

---

## âœ… Functional Requirements

| ID | Requirement | Status |
|----|-------------|:------:|
| FR1-FR5 | Multi-input support (image, audio, text) | âœ… |
| FR6-FR8 | Facial emotion recognition with face detection | âœ… |
| FR9-FR10 | Speech emotion analysis from audio | âœ… |
| FR11-FR12 | Text emotion analysis using NLP | âœ… |
| FR13-FR14 | Multimodal fusion with adaptive weighting | âœ… |
| FR15-FR17 | Web dashboard with visualization | âœ… |
| FR19-FR20 | Model training and evaluation pipeline | âœ… |
| FR22-FR23 | Comprehensive documentation | âœ… |

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [QUICKSTART.md](QUICKSTART.md) | Step-by-step setup guide |
| [DATASET_INSTRUCTIONS.md](DATASET_INSTRUCTIONS.md) | How to download and organize datasets |
| [Technical Documentation](docs/TECHNICAL_DOCUMENTATION.md) | In-depth technical details |
| [User Guide](docs/USER_GUIDE.md) | How to use the dashboard |
| [Deployment Guide](docs/DEPLOYMENT_GUIDE.md) | Production deployment options |

---

## ğŸ“„ License

This project is developed as part of an academic research initiative at the university level.

## ğŸ™ Acknowledgments

- **Datasets:** FER2013, RAVDESS, GoEmotions
- **Pre-trained Models:** BERT (Google), EfficientNet (Google)
- **Libraries:** TensorFlow, PyTorch, HuggingFace, Streamlit, librosa, OpenCV
- **Research Papers:**
  - *"Real-time Convolutional Neural Networks for Emotion and Gender Classification"* â€” MiniXception architecture
  - *"EfficientNetV2: Smaller Models and Faster Training"* (Tan & Le, 2021)
  - *"BERT: Pre-training of Deep Bidirectional Transformers"* (Devlin et al., 2019)
  - *"Attention Is All You Need"* (Vaswani et al., 2017) â€” Transformer/attention mechanism
