# üéì Student's Emotion Recognition using Multimodality and Deep Learning

> An AI-powered system that **automatically detects how a student is feeling** by analyzing their **face**, **voice**, and **written words** ‚Äî all at the same time ‚Äî and combines the results into one accurate emotion reading.

---

## üìã Table of Contents

- [What Does This Project Do?](#-what-does-this-project-do)
- [Why Is This Useful?](#-why-is-this-useful)
- [How Does It Work? (Simple Explanation)](#-how-does-it-work-simple-explanation)
- [The 3 AI Models Explained](#-the-3-ai-models-explained)
- [How Results Are Combined](#-how-results-are-combined)
- [Project Structure](#-project-structure)
- [How to Install and Run](#-how-to-install-and-run)
- [How to Use the Dashboard](#-how-to-use-the-dashboard)
- [Model Performance](#-model-performance)
- [Technology Used](#-technology-used)
- [Datasets Used for Training](#-datasets-used-for-training)

---

## üéØ What Does This Project Do?

This system **reads a student's emotion** using three different methods simultaneously:

| Input Method | What It Analyzes | Example |
|---|---|---|
| üì∑ **Face Photo / Camera** | Facial expressions | Is the student smiling? Frowning? Looking scared? |
| üé§ **Voice / Audio Recording** | Tone, pitch, energy of speech | Is the voice loud and angry? Quiet and sad? |
| üìù **Text / Typed Message** | Meaning of written words | "I love this class!" vs "I hate this assignment" |

It can detect **6 emotions**:

| Emotion | What It Looks Like |
|---|---|
| üòä **Happy** | Smiling face, cheerful voice, positive words |
| üò¢ **Sad** | Downturned lips, slow quiet voice, negative words |
| üò† **Angry** | Furrowed brows, loud harsh voice, aggressive words |
| üòê **Neutral** | Relaxed face, calm voice, factual statements |
| üò® **Fear** | Wide eyes, trembling voice, anxious words |
| üò≤ **Surprise** | Raised eyebrows, sudden pitch change, unexpected news |

---

## üí° Why Is This Useful?

**The Problem:**
- Teachers in large classrooms cannot monitor every student's emotions
- Online learners often feel disconnected and their struggles go unnoticed
- A student might *say* they are fine but *look* confused or sad

**Our Solution:**
- This AI monitors student emotions automatically in real-time
- By combining face + voice + text, it is far more accurate than using just one method
- Educators can use this data to identify struggling students early and offer help

**Example of Why Multiple Methods Matter:**

> Imagine a student is smiling üòä (face says: happy)  
> but says "Yeah right, this makes total sense" in a sarcastic tone üò† (voice says: angry)  
> and types "I don't understand anything" üò¢ (text says: sad)  
>
> A system using only the face would say "Happy" ‚Äî completely wrong!  
> Our system **combines all three** and correctly identifies the student is **frustrated**.

---

## üß† How Does It Work? (Simple Explanation)

Think of it like three expert judges each watching the same student:

```
üëÅÔ∏è JUDGE 1 (Face Expert)     ‚Üí Looks at the face photo       ‚Üí Says "I think: HAPPY (85%)"
üé§ JUDGE 2 (Voice Expert)     ‚Üí Listens to the audio          ‚Üí Says "I think: HAPPY (92%)"
üìù JUDGE 3 (Text Expert)      ‚Üí Reads what student wrote      ‚Üí Says "I think: HAPPY (98%)"
                                                                          ‚Üì
                                                            üßÆ COMBINE ALL THREE
                                                                          ‚Üì
                                                         ‚úÖ FINAL ANSWER: HAPPY (89%)
```

**Step-by-step flow:**

1. **You provide input** ‚Äî upload a photo, record audio, or type some text (you can use all three or just one)
2. **Three AI models independently analyze** each type of input
3. **Each model gives its prediction** ‚Äî e.g., "I'm 80% sure this is 'happy'"
4. **A Fusion Engine combines all predictions** ‚Äî it's smart enough to trust the more confident model more
5. **One final answer is given** ‚Äî the most likely emotion with a confidence percentage

---

## üî¨ The 3 AI Models Explained

### üëÅÔ∏è Model 1 ‚Äî Facial Emotion Recognition (CNN)

**What is a CNN?**
A CNN (Convolutional Neural Network) is a type of AI that was designed to **understand images**, just like how your eyes and brain process visual information.

**How it works:**
1. The system first **detects the face** in the photo using a face detector (like a smartphone's face scanner)
2. The face is **converted to grayscale** (black & white) and shrunk to a tiny 48√ó48 pixel image
3. The CNN **scans the image in small patches** ‚Äî looking for patterns like:
   - Curved lips (smile) ‚Üí likely happy
   - Furrowed eyebrows ‚Üí likely angry
   - Wide eyes ‚Üí likely surprised or scared
4. It outputs a **percentage score for each of the 6 emotions**

**Analogy:** Imagine teaching a child to recognize emotions by showing them thousands of faces with labels ‚Äî "this is happy," "this is sad." After enough examples, the child learns the patterns. That's exactly what this CNN did ‚Äî it was trained on **35,887 face images**.

---

### üé§ Model 2 ‚Äî Speech Emotion Recognition (BiLSTM with Attention)

**What is this model?**
This AI listens to audio and understands emotion from **HOW something is said**, not what words are used.

**Step 1 ‚Äî Feature Extraction (Converting Sound to Numbers)**

The audio file is analyzed to extract numerical features:
- **MFCC** (Mel-Frequency Cepstral Coefficients) ‚Äî captures the *tone and pitch* of the voice
- **Delta MFCC** ‚Äî captures how quickly the tone *changes* over time
- **Delta-Delta MFCC** ‚Äî captures the *acceleration* of those changes (like a sudden shout)

Think of it like this: instead of reading words, the AI reads the "shape" of the sound wave.

**Step 2 ‚Äî BiLSTM (Bidirectional Long Short-Term Memory)**

This part of the model reads the audio features **both forward and backward in time**:
- Forward: "After this quiet moment, the voice got louder" ‚Üí building anger?
- Backward: "Before the shout, there was a calm pause" ‚Üí deliberate emphasis?

Combined, it understands **the full emotional arc** of the speech.

**Step 3 ‚Äî Attention Mechanism (Focusing on What Matters)**

Not all parts of a speech clip are equally emotional. The attention mechanism focuses on the most expressive moments:

```
Time:     [0.1s]   [0.2s]   [0.3s]   [0.4s]   [0.5s]   [0.6s]
Audio:    "I..."   "just"   "can't"  "TAKE"    "this"   "anymore"
Focus:     5%       10%      15%       40%       15%       15%
                                        ‚Üë
                    Model pays MOST attention to the loud emotional word
```

**Trained on:** 1,440 audio clips from 24 professional actors (RAVDESS dataset)

---

### üìù Model 3 ‚Äî Text Emotion Recognition (BERT)

**What is BERT?**
BERT (Bidirectional Encoder Representations from Transformers) is a powerful AI from Google that was trained on the **entire English Wikipedia and thousands of books**. It deeply understands human language.

**How it works:**

Traditional language AI reads left to right: "I" ‚Üí "love" ‚Üí "this"  
BERT reads **all directions at once**, understanding context fully:

```
Sentence: "I can't believe how good this is!"

Traditional AI: reads word by word, might miss sarcasm
BERT:           understands "can't believe" + "how good" together = genuine excitement
```

**After BERT understands the text**, a classification layer converts that understanding into one of the 6 emotions.

**Trained on:** ~58,000 real Reddit comments (GoEmotions dataset by Google)

---

## üîÄ How Results Are Combined

This is called the **Fusion Engine** ‚Äî the brain that takes predictions from all 3 models and makes a final decision.

### The Smart Way (Calibrated Fusion ‚Äî Default)

**Problem with raw AI outputs:** AI models are often overconfident. For example, the face model might say "99% happy" when it's really more like "75% happy." This is called **miscalibration**.

**Step 1 ‚Äî Temperature Calibration (Fixing Overconfidence)**

Think of it like adjusting a car's speedometer that always reads 20% too high. We apply a correction factor called a "temperature" to make the predictions more realistic:

```
Before correction: Face says "99% happy"   ‚Üê Too confident
After correction:  Face says "78% happy"   ‚Üê More realistic
```

| Model | Temperature Applied | Why |
|---|---|---|
| Face | 1.5 | Most overconfident ‚Äî needs the most correction |
| Speech | 1.3 | Moderately overconfident |
| Text | 1.2 | Slightly overconfident |

**Step 2 ‚Äî Confidence Gating (Ignoring Unreliable Inputs)**

If one model is very unsure (less than 30% confident), it gets **completely skipped** in the final calculation. Why take advice from someone who says "I'm only 20% sure"?

**Step 3 ‚Äî Weighted Combination**

Each model has a base importance (weight). The final answer is a weighted average:

| Modality | Base Weight | Why |
|---|---|---|
| üòä Face | 40% | Facial expressions are the strongest signal for basic emotions |
| üé§ Speech | 30% | Voice tone carries a lot of emotional information |
| üìù Text | 30% | Words convey meaning but can sometimes be ambiguous |

**Example Calculation:**
```
Face:   happy=80%, confidence=0.80 ‚Üí effective weight = 0.40 √ó 0.80 = 0.32
Speech: happy=90%, confidence=0.95 ‚Üí effective weight = 0.30 √ó 0.95 = 0.285
Text:   happy=20%, confidence=0.25 ‚Üí SKIPPED (below 30% threshold)

Final emotion = weighted average of face + speech only ‚Üí HAPPY ‚úÖ
```

### Other Available Fusion Strategies

| Strategy | How It Works | Best For |
|---|---|---|
| **Calibrated** (Default) | Smart ‚Äî fixes overconfidence + ignores bad inputs | Most accurate, production use |
| **Weighted** | Simple average with fixed weights | Quick, predictable results |
| **Adaptive** | Weights change based on how confident each model is | Variable quality inputs |
| **Voting** | Each model votes, majority wins | Fast consensus decisions |

---

## üìÅ Project Structure

Here is every file and folder explained in plain English:

```
Student's Emotion Recognition/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ run_dashboard.py          ‚Üê ‚≠ê THE MAIN FILE ‚Äî Run this to start the app!
‚îú‚îÄ‚îÄ üìÑ download_models.py        ‚Üê ‚≠ê Run this FIRST to download the AI models
‚îú‚îÄ‚îÄ üìÑ requirements.txt          ‚Üê List of all Python libraries needed
‚îú‚îÄ‚îÄ üìÑ README.md                 ‚Üê This file you are reading now
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                      ‚Üê All the source code (the brain of the system)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config.py             ‚Üê Global settings (model paths, emotion labels, etc.)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ facial_recognition/   ‚Üê Everything related to analyzing faces
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_architecture.py   ‚Üê Defines the CNN neural network structure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotion_model.py        ‚Üê Loads & runs the facial model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ face_detector.py        ‚Üê Detects faces in photos (like auto-focus)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py   ‚Üê Prepares images for training
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py                ‚Üê Script to train the facial model
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ speech_analysis/      ‚Üê Everything related to analyzing voice
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotion_model.py        ‚Üê The BiLSTM model for voice emotion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio_features.py       ‚Üê Extracts MFCC features from audio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ speech_recognition.py   ‚Üê Audio file loading utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py                ‚Üê Script to train the speech model
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ text_analysis/        ‚Üê Everything related to analyzing text
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotion_model.py        ‚Üê The BERT model for text emotion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_preprocessing.py   ‚Üê Cleans and tokenizes text input
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py                ‚Üê Script to train the text model
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ fusion/               ‚Üê Combines predictions from all 3 models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multimodal_fusion.py    ‚Üê The 4 fusion strategies + calibration logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multimodal_predictor.py ‚Üê Easy-to-use API to get predictions
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ dashboard/            ‚Üê The web interface (what you see in the browser)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app.py               ‚Üê Streamlit dashboard ‚Äî the visual front-end
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/                ‚Üê Helper utilities
‚îÇ       ‚îú‚îÄ‚îÄ helpers.py           ‚Üê Common helper functions
‚îÇ       ‚îî‚îÄ‚îÄ voice_recorder.py    ‚Üê Tool for recording audio directly in the app
‚îÇ
‚îú‚îÄ‚îÄ üìÅ saved_models/             ‚Üê Where the trained AI models are stored
‚îÇ   ‚îú‚îÄ‚îÄ facial_emotion_model.h5  ‚Üê Facial CNN model (~43 MB)
‚îÇ   ‚îú‚îÄ‚îÄ speech_emotion_model.h5  ‚Üê Speech BiLSTM model (~6 MB)
‚îÇ   ‚îî‚îÄ‚îÄ text_bert_model/         ‚Üê BERT text model folder (~438 MB)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                     ‚Üê Training datasets (not included ‚Äî too large)
‚îú‚îÄ‚îÄ üìÅ docs/                     ‚Üê Technical documentation and training charts
‚îî‚îÄ‚îÄ üìÅ tests/                    ‚Üê Automated tests for code quality
```

---

## üöÄ How to Install and Run

> ‚ö†Ô∏è **Before starting** ‚Äî Make sure you have **Python 3.8 or newer** installed on your computer.  
> Check by opening your terminal/command prompt and typing: `python3 --version`  
> If you see a version number like `Python 3.10.x`, you are good to go!

---

### Step 1 ‚Äî Download (Clone) the Project

**What does "clone" mean?**  
Cloning means copying all the project files from the internet (GitHub) to your computer.

Open your **Terminal** (Mac/Linux) or **Command Prompt** (Windows) and type:

```bash
git clone https://github.com/srivardhan-kondu/Student-s-Emotion-Recognition-using-Multimodality-and-Deep-Learning.git
```

Then navigate into the project folder:

```bash
cd "Student's Emotion Recognition using Multimodality and Deep Learning"
```

> üí° **What is Git?** Git is a tool for downloading and managing code. If you don't have it,  
> download it from [https://git-scm.com/downloads](https://git-scm.com/downloads) and install it first.

---

### Step 2 ‚Äî Create a Virtual Environment

**What is a virtual environment?**  
Think of it like a separate clean room for this project's software. It prevents this project's libraries from mixing with other Python projects on your computer.

**On Mac / Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**On Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

> ‚úÖ **How do you know it worked?**  
> You will see `(venv)` appear at the start of your terminal line, like:  
> `(venv) your-computer:project $`

---

### Step 3 ‚Äî Install Required Libraries

**What are libraries?**  
Libraries are pre-built tools that our code uses. For example, TensorFlow (for AI), OpenCV (for image processing), etc.

Run this command ‚Äî it will automatically install everything:

```bash
pip install -r requirements.txt
```

> ‚è≥ This may take **5‚Äì15 minutes** depending on your internet speed.  
> You will see text scrolling on screen ‚Äî that's normal, it's downloading and installing.

Then install some language processing data:

```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
```

---

### Step 4 ‚Äî Download the Pre-trained AI Models ‚ö°

**Why is this step needed?**  
The AI models are the "brains" of the system. They have already been trained (which took hours of computing time), so you don't have to train them yourself. We just need to download the finished, ready-to-use models.

Run:
```bash
python download_models.py
```

> ‚è≥ This will download ~490 MB total from Google Drive. Time depends on your internet speed.  
> You will see a progress bar for each model file.

What gets downloaded into your `saved_models/` folder:

| File | What It Is | Size |
|---|---|---|
| `facial_emotion_model.h5` | Trained facial expression AI | ~43 MB |
| `speech_emotion_model.h5` | Trained voice tone AI | ~6 MB |
| `text_bert_model/` | Trained text understanding AI | ~438 MB |

> ‚úÖ When it finishes you will see:  
> `‚úÖ All models downloaded successfully!`  
> `üöÄ You can now run: python run_dashboard.py`

## ‚ö†Ô∏è Important: If ZIP Models Do Not Extract Correctly

In some systems, the Google Drive ZIP file for the BERT model may not
extract properly using the automatic script.

If this happens, follow the steps below carefully.

------------------------------------------------------------------------

### Step 1 --- Download the Model Manually

If `download_models.py` fails to extract the BERT model correctly:

1.  Open the Google Drive link shown in the terminal manually.
2.  Download the `text_bert_model.zip` file.
3.  Extract it inside the `saved_models/` folder.

------------------------------------------------------------------------

### Step 2 --- Fix Folder Structure (Very Important)

After extraction, you might see this incorrect folder structure:

saved_models/ ‚îî‚îÄ‚îÄ text_bert_model/ ‚îî‚îÄ‚îÄ text_bert_model/ ‚îú‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ pytorch_model.bin ‚îú‚îÄ‚îÄ tokenizer.json ‚îî‚îÄ‚îÄ other model files

‚ö†Ô∏è This structure is incorrect and will cause the application to fail
when loading the text model.

You must modify it so it becomes:

saved_models/ ‚îî‚îÄ‚îÄ text_bert_model/ ‚îú‚îÄ‚îÄ config.json ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îú‚îÄ‚îÄ tokenizer.json ‚îî‚îÄ‚îÄ other model files

------------------------------------------------------------------------

### How to Fix the Structure

1.  Open the inner `text_bert_model` folder.
2.  Move all files one level up into the outer `text_bert_model` folder.
3.  Delete the now-empty inner folder.

After correcting the structure, the dashboard will load the text model
successfully.


---

### Step 5 ‚Äî Launch the Dashboard! üéâ

```bash
python run_dashboard.py
```

You will see this message in the terminal:
```
üöÄ Starting Multimodal Emotion Recognition Dashboard...
üìç Dashboard will be available at: http://localhost:8501
```

Now open your web browser (Chrome, Firefox, etc.) and go to:

## üëâ [http://localhost:8501](http://localhost:8501)

The dashboard will open and you're ready to use it!

> üõë **To stop the server:** Press `Ctrl + C` in the terminal.

---

> <details>
> <summary>üîß Advanced: Want to retrain the models yourself? (Optional ‚Äî takes hours)</summary>
>
> You only need this if you want to train from scratch using your own data.
>
> **Download the datasets:**
>
> | Dataset | For | Size | Download |
> |---|---|---|---|
> | FER2013 | Face model | ~300 MB | [Kaggle](https://www.kaggle.com/datasets/msambare/fer2013) |
> | RAVDESS | Speech model | ~1.1 GB | [Kaggle](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio) |
> | GoEmotions | Text model | ~50 MB | [Kaggle](https://www.kaggle.com/datasets/debarshichanda/goemotions) |
>
> Place them in: `data/facial/fer2013/`, `data/speech/ravdess/`, `data/text/goemotions/`
>
> Then run:
> ```bash
> python src/facial_recognition/train.py   # ~30 min on GPU
> python src/speech_analysis/train.py      # ~20 min on GPU
> python src/text_analysis/train.py        # ~40 min on GPU
> ```
> </details>

---

## üñ•Ô∏è How to Use the Dashboard

When you open [http://localhost:8501](http://localhost:8501), you will see a dashboard with **4 tabs**:

### Tab 1 ‚Äî üéØ Multimodal (Most Powerful)

This tab uses **all three methods at once** for the most accurate result.

What you can do:
1. üì∑ Upload a photo of the student's face (JPG or PNG)
2. üé§ Upload an audio recording of the student speaking (WAV or MP3)
3. üìù Type or paste a message the student wrote
4. Click **"Analyze Emotion"**
5. See the combined result with individual scores from each model

### Tab 2 ‚Äî üëÅÔ∏è Image Only

Upload **just a photo** to detect emotion from the face alone.  
Useful when you only have a webcam image.

### Tab 3 ‚Äî üé§ Audio Only

Upload **just an audio file** to detect emotion from voice alone.  
Useful for analyzing voice recordings or phone calls.

### Tab 4 ‚Äî üìù Text Only

Type or paste **any text** to detect emotion from words alone.  
Useful for analyzing student chat messages or written feedback.

---

### Sidebar Options (Left Panel)

| Option | What It Does |
|---|---|
| **Fusion Strategy** | Choose how the 3 models are combined (Calibrated is recommended) |
| **Modality Weights** | Adjust how much weight each model gets (Face / Speech / Text sliders) |
| **Prediction History** | See the last 5 predictions made |

---

## üìä Model Performance

How accurate is each AI model?

| Model | Dataset Used | Number of Samples | Accuracy |
|---|---|---|---|
| üëÅÔ∏è Facial CNN | FER2013 | 35,887 images | **57.7%** |
| üé§ Speech BiLSTM | RAVDESS | 1,440 audio clips | **97.0%** |
| üìù Text BERT | GoEmotions | 58,000 text samples | **65.9%** |

> **Why is facial accuracy lower?**  
> The FER2013 dataset (used for faces) is notoriously difficult ‚Äî even humans only agree ~65% of the time on these images. Our model at 57.7% is typical for lightweight models. More complex systems achieve ~73% but require 10x more computing power.
>
> **Why is speech accuracy so high?**  
> The RAVDESS dataset uses professional actors with *very clear* emotional expressions. Real-world audio would be harder.

---

## üõ†Ô∏è Technology Used

| Category | Tool | What It's Used For |
|---|---|---|
| **AI Framework** | TensorFlow / Keras | Building and running the Face and Speech models |
| **AI Framework** | PyTorch | Building and running the BERT text model |
| **Language AI** | HuggingFace Transformers | BERT model library |
| **Face Detection** | OpenCV (Haar Cascade) | Detecting faces in photos |
| **Audio Processing** | librosa | Extracting audio features (MFCC) |
| **Text Processing** | NLTK | Cleaning and preparing text |
| **Web Dashboard** | Streamlit | Building the interactive browser interface |
| **Charts** | Plotly / Matplotlib | Showing emotion charts and graphs |
| **Data Science** | NumPy, Pandas, scikit-learn | Data manipulation and evaluation |

---

## üì¶ Datasets Used for Training

### üëÅÔ∏è FER2013 (For the Face Model)

| Property | Details |
|---|---|
| Total images | 35,887 face photos |
| Image size | 48 √ó 48 pixels, black & white |
| Source | Kaggle / Facial Expression Recognition Challenge |
| Emotions | 7 (we use 6) |

### üé§ RAVDESS (For the Speech Model)

| Property | Details |
|---|---|
| Total clips | 1,440 audio recordings |
| Speakers | 24 professional actors (12 male, 12 female) |
| Audio format | WAV, 48,000 samples per second |
| Source | Ryerson University Audio-Visual Database |

### üìù GoEmotions (For the Text Model)

| Property | Details |
|---|---|
| Total texts | ~58,000 Reddit comments |
| Original labels | 27 emotions (we map to 6) |
| Language | English |
| Source | Google Research |

---

## üìö Additional Documentation

| Document | What's Inside |
|---|---|
| [QUICKSTART.md](QUICKSTART.md) | Even shorter setup guide |
| [DATASET_INSTRUCTIONS.md](DATASET_INSTRUCTIONS.md) | How to download datasets (for retraining) |
| [docs/TECHNICAL_DOCUMENTATION.md](docs/TECHNICAL_DOCUMENTATION.md) | Deep technical details for developers |
| [docs/USER_GUIDE.md](docs/USER_GUIDE.md) | Detailed user guide for the dashboard |
| [docs/DEPLOYMENT_GUIDE.md](docs/DEPLOYMENT_GUIDE.md) | How to deploy on a server |

---

## üìÑ License

This project is developed as part of an academic research initiative at the university level.

## üôè Acknowledgments

- **Datasets:** FER2013, RAVDESS, GoEmotions
- **Pre-trained Models:** BERT (Google), EfficientNet (Google)
- **Libraries:** TensorFlow, PyTorch, HuggingFace, Streamlit, librosa, OpenCV
- **Research Papers:**
  - *"Real-time Convolutional Neural Networks for Emotion and Gender Classification"* ‚Äî MiniXception architecture
  - *"BERT: Pre-training of Deep Bidirectional Transformers"* (Devlin et al., 2019)
  - *"Attention Is All You Need"* (Vaswani et al., 2017)
